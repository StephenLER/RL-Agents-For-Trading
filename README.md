## 1) 论文要解决什么问题

目标：给定一个股票池（论文用DJIA 30只），设计一个自动交易策略，使投资组合的**净资产长期增长**且更稳健（风险调整后收益好）。论文用**深度强化学习**训练交易智能体，并用**集成策略**在不同市场环境下动态选择表现最好的算法（用 Sharpe 比率选）。

---

## 2) 把交易建模成强化学习（MDP）

论文把多股票交易写成一个MDP（马尔可夫决策过程）：每个交易日是一步。

### 2.1 状态 State (s_t)

论文状态包含：

* (b_t)：现金余额
* (p_t)：每只股票的价格向量（调整后收盘价）
* (h_t)：每只股票的持仓股数
* 以及技术指标：MACD/RSI/CCI/ADX（各一条向量）
  所以整体是 ([b_t,p_t,h_t, M_t,R_t,C_t,X_t])。

**原理**：RL要做决策必须看到“当前可用信息”。价格+持仓决定你现在的资产结构；指标提供趋势/超买超卖等信号，让策略不只看价格本身。

### 2.2 动作 Action (a_t)

对每只股票给一个动作：

* 允许买、卖、持有，等价于“买/卖多少股”。
  论文把每只股票动作设为 ({-k,\dots,0,\dots,k})，其中 (k\le h_{max})，并把动作**归一化到 [-1,1]**以适配连续动作算法（PPO/A2C的高斯策略、DDPG的连续动作）。

**原理**：

* 多股票组合如果用离散动作会爆炸（((2k+1)^{30})太大）。
* 用连续动作等价于输出“强度/比例”，再映射到交易股数，更可训练。

### 2.3 约束：不能负余额 + 交易成本 + 风险控制

* **不能负余额**：动作必须让下一步余额 (b_{t+1}\ge0)。论文给了约束式：卖出获得现金、买入消耗现金。
* **交易成本**：每次交易按成交额的比例收取。论文示例用了0.1%。
* **风险控制：湍流指数 turbulence**：衡量市场“极端异常波动”。当 turbulence 超阈值，停止买入并卖出全部持仓。

**原理**：现实市场有崩盘/极端行情，纯最大化收益会在极端波动时爆仓。湍流相当于一个“风险开关”：极端时强制降风险。

### 2.4 奖励 Reward (r_t)

奖励就是**净资产的增量**（扣除交易成本后）：
[
r(s_t,a_t,s_{t+1})=(b_{t+1}+p_{t+1}^T h_{t+1})-(b_t+p_t^T h_t)-c_t
]

**原理**：
这是最直接的“你今天做的决策让组合价值涨了多少”。RL的目标就是最大化长期累计奖励，也就是最大化长期财富增长。

---

## 3) 数据处理流程（训练前必须做）

论文的数据处理思路是：准备日频数据 → 计算技术指标 → 计算湍流指数 → 按时间分训练/验证/交易。

你用A股时要额外处理停牌/缺失：对齐每一天都有全部股票（你代码里 align_data+ffill 就是在做这件事）。

---

## 4) 训练流程（论文的“主流程”）

论文训练不是一次训练完就用，而是一个**滚动的 train→validate→trade→再训练**流程（集成策略的一部分）。

### Step A：训练（Training window）

用过去一段时间的数据训练三种算法（PPO/A2C/DDPG）。

### Step B：验证（Validation window）

接着用一段验证窗口跑三种模型，算 Sharpe 比率，挑表现最好的那一个。

### Step C：交易（Trading window）

用选出来的最佳模型在下一段交易窗口执行交易（做预测/下单）。

### Step D：滚动

每隔固定周期（论文是每3个月）滚动窗口：扩大/更新训练集，再训练三模型，再验证，再选。

**为什么要滚动？**
市场状态会变：有时趋势行情，有时震荡/熊市。单一算法容易“只擅长一种市场”。滚动验证挑Sharpe最高，相当于让系统“适配当前市场”。

---

## 5) 三种算法的原理与它们怎么训练

这三者都属于 actor-critic（DDPG更像确定性actor-critic）。共同点：

* **Actor（策略网络）**：给定状态输出动作
* **Critic（价值网络）**：评估这个状态/动作好不好（价值或Q值）
  然后用 critic 指导 actor 更新。

### 5.1 A2C：Advantage Actor-Critic

论文强调A2C通过“优势函数 advantage”降低策略梯度方差、训练更稳。

**核心公式**（论文给出）：
策略梯度目标：
[
\nabla J(\theta)=\mathbb{E}\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t),A(s_t,a_t)\right]
]

优势函数：
[
A(s_t,a_t)=Q(s_t,a_t)-V(s_t)
]
或用TD近似：
[
A(s_t,a_t)=r_t+\gamma V(s_{t+1})-V(s_t)
]

**训练怎么做（直观版）**：

1. 用当前策略在环境里跑一段轨迹（多步）
2. Critic用回报/TD误差学 (V(s))（值函数）
3. Actor用 advantage 更新：如果某动作比“平均水平”更好（A>0）就增加它概率，反之减少
4. A2C通常用并行环境收集样本并同步更新（比A3C更简单更稳定）

**适合交易的原因**：相对稳定，风险控制好（论文也说A2C更偏“抗风险”）。

---

### 5.2 DDPG：Deep Deterministic Policy Gradient

DDPG是为**连续动作**设计的：输出确定性动作 (a=\mu(s))；用Q网络评估。

**关键机制**：经验回放 replay buffer + 目标网络 target networks。论文描述：把转移 ((s_t,a_t,r_t,s_{t+1})) 存入回放池，从中采样小批量更新网络。

论文给了 critic 的目标：
[
y_i=r_i+\gamma Q'(s_{i+1},\mu'(s_{i+1}))
]

critic 损失：
[
L(\theta^Q)=\mathbb{E}[(y_i-Q(s_i,a_i))^2]
]

**Actor更新**（论文没在这一页展开，但标准DDPG是最大化Q）：让策略输出的动作在critic看来更高Q。

**训练怎么做（直观版）**：

1. Actor给动作 (a_t=\mu(s_t))，为了探索加噪声（你代码里的 NormalActionNoise 正是这一步）
2. 环境返回 (r_t,s_{t+1})，存入 replay buffer
3. 从 buffer 采样一批，更新 critic 拟合 (y_i)
4. 用 critic 的梯度指导 actor：让 (\mu(s)) 输出让Q更高的动作
5. 用 target network 平滑训练，避免发散

**适合交易的原因**：动作连续控制精细；但也更容易不稳定，需要噪声探索和回放池稳定学习。

---

### 5.3 PPO：Proximal Policy Optimization

PPO解决“策略梯度更新太猛导致崩”的问题：用 **clipping** 限制新旧策略差异。论文给出概率比：
[
r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
]

PPO的剪切目标：
[
J_{CLIP}(\theta)=\hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t\right)\right]
]

**训练怎么做（直观版）**：

1. 用当前策略采样轨迹（on-policy）
2. 估计 advantage (\hat{A}_t)（通常用GAE或TD）
3. 更新策略时，如果新策略把概率改动过大，就被clip限制，避免一步走偏
4. 多个epoch对同一批数据做小步更新（稳定、好调参）

**适合交易原因**：稳定、收敛快、实现简单。论文也强调PPO“稳定、快、易调”。

---

## 6) 集成策略（Ensemble）到底怎么做

论文不是把三模型动作平均，而是“**动态模型选择**”：每个周期都选 Sharpe 最高的模型去交易下一段。步骤在论文第5节D：

1. 每3个月滚动训练三模型
2. 用后续3个月验证窗口算三模型 Sharpe，选最高的
3. 用选中的模型交易下一个季度
   Sharpe公式也给了：

**原理**：

* Sharpe衡量“单位风险收益”，不是只看赚不赚，而是看稳不稳。
* 不同算法对不同市场更擅长：论文观察到A2C更抗风险，PPO更追趋势，DDPG作为补充。
* 用验证集“最近表现”选模型，相当于一种自适应机制。

---

## 7) 你复现时“训练部分”应当长什么样

严格照论文，你训练代码最终会变成：

1. 先做一次初始训练（或用第一个训练窗口训练三模型）
2. 验证三模型 → 记录Sharpe → 选模型
3. 用选中模型跑下一段交易窗口
4. 滚动：更新训练窗口（增长或滑动）→ 重训三模型 → 再选

